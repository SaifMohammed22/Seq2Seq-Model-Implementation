{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d57526c",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0050897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd13121",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb1227",
   "metadata": {},
   "source": [
    "Data Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f1567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = 0\n",
    "EOS = 1\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b6d6b",
   "metadata": {},
   "source": [
    "Convert Unicode (U+0041) -> ASCII (65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c0337b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = UnicodeToAscii(s.strip().lower())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1fedd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLang(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines....\")  \n",
    "    lines = open(f\"data/{lang1}-{lang2}.txt\").read().strip().split(\"\\n\")\n",
    "    \n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)   \n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "10b047dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefix = (\n",
    "    \"i am\", \"i m\",\n",
    "    \"he is\", \"he s\",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re\",\n",
    "    \"we are\", \"we re\",\n",
    "    \"they are\", \"they re\",\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and \\\n",
    "           len(p[1].split(\" \")) < MAX_LENGTH and \\\n",
    "           p[1].startswith(eng_prefix)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b0b0d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines....\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 12892 sentence pairs\n",
      "Counting Words...\n",
      "Counted Words:\n",
      "fra 5228\n",
      "eng 3434\n",
      "['ils sont avec moi', 'they re with me']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLang(lang1, lang2, reverse)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting Words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted Words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\", True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd846d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae61fa4",
   "metadata": {},
   "source": [
    "Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "983753a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        return output, hidden\n",
    "        \n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "    \n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c1eb7",
   "metadata": {},
   "source": [
    "---\n",
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "76abf504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2idx[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, paris = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS)\n",
    "        tgt_ids.append(EOS)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, : len(tgt_ids)] = tgt_ids\n",
    "    \n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "\n",
    "    train_sample = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sample, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "050b0c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 5000])\n",
      "torch.Size([320, 5000])\n"
     ]
    }
   ],
   "source": [
    "decoder = torch.rand(32, 10, 5000)\n",
    "print(decoder.shape)\n",
    "new_decoder = decoder.view(-1, decoder.size(-1))\n",
    "print(new_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e044b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor= data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        # Most loss function expect:\n",
    "        # Prediction: (N, C) where N = the number of samples, C = number of classes\n",
    "        # Target/Ground truth: (N,) where N = number of samples\n",
    "        # decoder_outputs: (batch_size, seq_len, vocab_len) -> (batch_size * seq_len, vocab_len)\n",
    "        # target_tensor: (batch_size, seq_len)-> (batch_size * seq_len,)\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a846afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af97f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, epochs, lr=0.001, print_every=100, plot_every=100):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} ({epoch/epochs*100:5.1f}%) | Avg Loss: {print_loss_avg:.4f}\")\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e286e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_id = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for idx in decoded_id:\n",
    "            if idx.item() == EOS:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            decoded_words.append(output_lang.idx2word[idx.item()])\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a261ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\">\", pair[0])\n",
    "        print(\"=\", pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"<\", output_sentence)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbbf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines....\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 12892 sentence pairs\n",
      "Counting Words...\n",
      "Counted Words:\n",
      "fra 5228\n",
      "eng 3434\n",
      "Epoch   5/80 (  6.2%) | Avg Loss: 2.2408\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_loader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_loader, encoder, decoder, epochs=80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab2dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis en vacances\n",
      "= i m on holiday\n",
      "< i m the one <EOS>\n",
      "\n",
      "> il est cense etre chez lui aujourd hui\n",
      "= he is supposed to be at home today\n",
      "< he is used to be in to <EOS>\n",
      "\n",
      "> il me faut trouver de nouvelles amies\n",
      "= i must find some new friends\n",
      "< i must find new new new <EOS>\n",
      "\n",
      "> vous avez tort\n",
      "= you are wrong\n",
      "< you re wrong <EOS>\n",
      "\n",
      "> elle est une vrai beaute\n",
      "= she is a real beauty\n",
      "< she is a doctor beautiful <EOS>\n",
      "\n",
      "> nous sommes pareils\n",
      "= we re the same\n",
      "< we re the here <EOS>\n",
      "\n",
      "> j ai emis des reserves\n",
      "= i made reservations\n",
      "< i made you <EOS>\n",
      "\n",
      "> je ne suis toujours pas impressionne\n",
      "= i m still not impressed\n",
      "< i m not not either <EOS>\n",
      "\n",
      "> nous sommes tres reconnaissants pour votre hospitalite\n",
      "= we re very grateful for your hospitality\n",
      "< we re very grateful for your hospitality <EOS>\n",
      "\n",
      "> il tira sur l oiseau mais le manqua\n",
      "= he shot at the bird but missed it\n",
      "< he saw the the bird the the <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f095b",
   "metadata": {},
   "source": [
    "----\n",
    "Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd92a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/saif/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1934633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_blue(encoder, decoder, pairs, input_lang, output_lang, n_samples=100):\n",
    "    reference = []\n",
    "    hypotheses = []\n",
    "\n",
    "    test_paris = random.sample(pairs, min(n_samples, len(pairs)))\n",
    "\n",
    "    for pair in test_paris:\n",
    "        reference = pair[1].split() #target\n",
    "        reference.append([reference])\n",
    "\n",
    "        predicted_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        if \"<EOS>\" in predicted_words:\n",
    "            predicted_words = predicted_words[:predicted_words.index(\"<EOS>\")]\n",
    "        hypotheses.append(predicted_words)\n",
    "    \n",
    "    smoothie = SmoothingFunction().method4\n",
    "    blue_score = corpus_bleu(reference, hypotheses, smoothing_function=smoothie)\n",
    "\n",
    "    return blue_score * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df93a4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((10,20))\n",
    "B = torch.randn((10,20))\n",
    "C = torch.randn((10,20))\n",
    "\n",
    "\n",
    "W = torch.hstack([A, B, C])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f44feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
